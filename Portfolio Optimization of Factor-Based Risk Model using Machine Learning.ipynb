{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Problem 0.\n",
    "All of the below pertain to the estimation universe as defined above. Modify the daily data frames,\n",
    "removing all non-estimation-universe rows, before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from statsmodels.formula.api import ols\n",
    "from scipy.stats import gaussian_kde\n",
    "import scipy\n",
    "import scipy.sparse\n",
    "from scipy.linalg import pinv\n",
    "import patsy\n",
    "from statistics import median\n",
    "import bz2\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/Users/kechengshi/Documents/Python/MATH5430/FACTOR_MODEL/'\n",
    "\n",
    "def sort_cols(test):\n",
    "    return(test.reindex(sorted(test.columns), axis=1))\n",
    "\n",
    "frames = {}\n",
    "for year in list(range(2003,2011)):\n",
    "    fil = model_dir + \"pandas-frames.\" + str(year) + \".pickle.bz2\"\n",
    "    frames.update(pd.read_pickle(fil))\n",
    "\n",
    "for x in frames:\n",
    "    frames[x] = sort_cols(frames[x])\n",
    "\n",
    "covariance = {}\n",
    "\n",
    "for year in list(range(2003,2011)):\n",
    "    fil = model_dir + \"covariance.\" + str(year) + \".pickle.bz2\"\n",
    "    covariance.update(pd.read_pickle(fil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_factors = ['AERODEF', 'AIRLINES', 'ALUMSTEL', 'APPAREL', 'AUTO',\n",
    "'BANKS','BEVTOB', 'BIOLIFE', 'BLDGPROD','CHEM', 'CNSTENG',\n",
    "'CNSTMACH', 'CNSTMATL', 'COMMEQP', 'COMPELEC',\n",
    "'COMSVCS', 'CONGLOM', 'CONTAINR', 'DISTRIB',\n",
    "'DIVFIN', 'ELECEQP', 'ELECUTIL', 'FOODPROD', 'FOODRET', 'GASUTIL',\n",
    "'HLTHEQP', 'HLTHSVCS', 'HOMEBLDG', 'HOUSEDUR','INDMACH', 'INSURNCE','INTERNET',\n",
    "'LEISPROD', 'LEISSVCS', 'LIFEINS', 'MEDIA', 'MGDHLTH','MULTUTIL',\n",
    "'OILGSCON', 'OILGSDRL', 'OILGSEQP', 'OILGSEXP',\n",
    "'PAPER', 'PHARMA', 'PRECMTLS','PSNLPROD','REALEST',\n",
    "'RESTAUR', 'ROADRAIL','SEMICOND', 'SEMIEQP','SOFTWARE',\n",
    "'SPLTYRET', 'SPTYCHEM', 'SPTYSTOR', 'TELECOM', 'TRADECO', 'TRANSPRT','WIRELESS']\n",
    "style_factors = ['BETA','SIZE','MOMENTUM','VALUE','LEVERAGE','LIQUIDTY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wins(x,a,b):\n",
    "    return(np.where(x <= a,a, np.where(x >= b, b, x)))\n",
    "def clean_nas(df):\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "    for numeric_column in numeric_columns:\n",
    "        df[numeric_column] = np.nan_to_num(df[numeric_column])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estu(df):\n",
    "    \"\"\"Estimation universe definition\"\"\"\n",
    "    estu = df.loc[df.IssuerMarketCap > 1e9].copy(deep=True)\n",
    "    return estu\n",
    "\n",
    "def colnames(X):\n",
    "    \"\"\" return names of columns, for DataFrame or DesignMatrix \"\"\"\n",
    "    if(type(X) == patsy.design_info.DesignMatrix):\n",
    "        return(X.design_info.column_names)\n",
    "    if(type(X) == pandas.core.frame.DataFrame):\n",
    "        return(X.columns.tolist())\n",
    "    return(None)\n",
    "\n",
    "def diagonal_factor_cov(date, X):\n",
    "    \"\"\"Factor covariance matrix, ignoring off-diagonal for simplicity\"\"\"\n",
    "    cv = covariance[date]\n",
    "    k = np.shape(X)[1]\n",
    "    Fm = np.zeros([k,k])\n",
    "    for j in range(0,k):\n",
    "        fac = colnames(X)[j]\n",
    "        Fm[j,j] = (0.01**2) * cv.loc[(cv.Factor1==fac) & (cv.Factor2==fac),\"VarCovar\"].iloc[0]\n",
    "    return(Fm)\n",
    "\n",
    "def risk_exposures(estu):\n",
    "    \"\"\"Exposure matrix for risk factors, usually called X in class\"\"\"\n",
    "    L = [\"0\"]\n",
    "    L.extend(style_factors)\n",
    "    L.extend(industry_factors)\n",
    "    my_formula = \" + \".join(L)\n",
    "    return patsy.dmatrix(my_formula, data = estu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter each daily frame for estimation universe\n",
    "for date, df in frames.items():\n",
    "    frames[date] = get_estu(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1.\n",
    "\n",
    "**Residual returns** Within each daily data frame, let **Y** denote the residuals of the variable **Ret**, with respect to the risk model. In other words, define\n",
    "\n",
    "$$\n",
    "Y := \\text{Ret} - XX^+\\text{Ret}\n",
    "$$\n",
    "\n",
    "where $X^+$ denotes the pseudoinverse, and $X$ is constructed as above (i.e., using the risk_exposures function). Augment the data frames you have been given, by adding a new column, $Y$, to each frame. Be sure to winsorize the *Ret* column prior to computing $Y$ as above. You do not have to save the augmented data, unless you want to. In other words, the modification that adds column $Y$ can be done in-memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, df in frames.items():\n",
    "    # Winsorize Ret\n",
    "    df[\"Ret\"] = wins(df[\"Ret\"], -0.25, 0.25)\n",
    "    \n",
    "    # Compute risk exposure matrix\n",
    "    X = risk_exposures(df)\n",
    "    \n",
    "    # Compute residual returns\n",
    "    df[\"Y\"] = df[\"Ret\"] - np.dot(np.dot(X, pinv(X)), df[\"Ret\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Problem 2.\n",
    "**Model selection** Split your data into a training/validation set $D_{\\text{train}}$, and an ultimate test set (vault), $D_{\\text{test}}$. Do not split within a single day; rather, some dates end up in $D_{\\text{train}}$ and the rest in $D_{\\text{test}}$. This will be the basis of your cross-validation study later on.\n",
    "\n",
    "It will be helpful to join together vertically the frames in the training/validation set $D_{\\text{train}}$ into a single frame called a panel. For the avoidance of doubt, the panel will have the same columns as any one of the daily frames individually, and the panel will have a large number of rows (the sum of all the rows of all the frames in $D_{\\text{train}}$).\n",
    "\n",
    "Consider list of candidate alpha factors given above. Find a model of the form\n",
    "\n",
    "$$\n",
    "Y = f(\\text{candidate alphas}) + \\epsilon\n",
    "$$\n",
    "\n",
    "where $Y$ is the residual return from above. Determine the function $f()$ using cross-validation to optimize any tunable hyper-parameters. First, to get started, assume $f$ is linear and use lasso or elastic net cross-validation tools (e.g. from sklearn). Then, get creative and try at least one non-linear functional form for $f$, again using cross-validation to optimize any tunable hyper-parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all daily frames into a single panel\n",
    "panel = pd.concat(frames.values(), keys=frames.keys(), names=[\"Date\", \"Index\"])\n",
    "del frames,df, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by dates: 80% training, 20% testing\n",
    "dates = panel.index.get_level_values(\"Date\").unique()\n",
    "train_dates = dates[:int(0.8 * len(dates))]\n",
    "test_dates = dates[int(0.8 * len(dates)):]\n",
    "\n",
    "# Create train and test sets\n",
    "train_panel = panel.loc[train_dates]\n",
    "test_panel = panel.loc[test_dates]\n",
    "\n",
    "# Candidate alpha factors\n",
    "candidate_alphas = ['STREVRSL', 'LTREVRSL', 'INDMOM', 'EARNQLTY', \n",
    "                    'EARNYILD', 'MGMTQLTY', 'PROFIT', 'SEASON', 'SENTMT']\n",
    "\n",
    "# Extract data for training\n",
    "Y_train = train_panel[\"Y\"].to_numpy()  # Residual returns\n",
    "X_train = np.asarray(train_panel[candidate_alphas])  # Alpha factors as a NumPy array=\n",
    "D_train = np.asarray((train_panel[\"SpecRisk\"] / (100 * math.sqrt(252))) ** 2)  # Specific risk diagonal\n",
    "\n",
    "# Extract data for testing\n",
    "Y_test = test_panel[\"Y\"].to_numpy()  # Residual returns\n",
    "X_test = np.asarray(test_panel[candidate_alphas])  # Alpha factors as a NumPy array\n",
    "D_test = np.asarray((test_panel[\"SpecRisk\"] / (100 * math.sqrt(252))) ** 2)  # Specific risk diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_wls_train(X, Y, D_diag, kappa=1e-5):\n",
    "    \"\"\"\n",
    "    Train a regularized WLS model with memory-efficient diagonal representation of D.\n",
    "    \n",
    "    Parameters:\n",
    "    X : ndarray\n",
    "        Feature matrix (n x p).\n",
    "    Y : ndarray\n",
    "        Target variable (n-dimensional vector).\n",
    "    D_diag : ndarray\n",
    "        Diagonal elements of the specific risk matrix (1D array).\n",
    "    kappa : float\n",
    "        Regularization parameter.\n",
    "        \n",
    "    Returns:\n",
    "    coef : ndarray\n",
    "        Estimated coefficients (p-dimensional vector).\n",
    "    \"\"\"\n",
    "    # Compute D^{-1} as element-wise inverse of D_diag\n",
    "    D_inv_diag = 1 / D_diag  # Inverse of diagonal elements\n",
    "\n",
    "    # Efficient computation of X^T D^{-1}\n",
    "    X_t_D_inv = (D_inv_diag[:, None] * X).T  # Element-wise multiplication\n",
    "    \n",
    "    # Regularization term\n",
    "    regularization = kappa * np.identity(X.shape[1])\n",
    "\n",
    "    # Compute coefficients\n",
    "    coef = np.linalg.inv(X_t_D_inv @ X + regularization) @ (X_t_D_inv @ Y)\n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_wls(X, coef):\n",
    "    \"\"\"\n",
    "    Make predictions using the WLS model.\n",
    "    \n",
    "    Parameters:\n",
    "    X : ndarray\n",
    "        Feature matrix (n x p).\n",
    "    coef : ndarray\n",
    "        Coefficients (p-dimensional vector).\n",
    "        \n",
    "    Returns:\n",
    "    predictions : ndarray\n",
    "        Predicted values (n-dimensional vector).\n",
    "    \"\"\"\n",
    "    return X @ coef\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Regularization parameter\n",
    "kappa = 1e-5\n",
    "\n",
    "# Train the model\n",
    "coefficients = regularized_wls_train(X_train, Y_train, D_train, kappa)\n",
    "\n",
    "# Make predictions\n",
    "Y_train_pred = predict_wls(X_train, coefficients)\n",
    "Y_test_pred = predict_wls(X_test, coefficients)\n",
    "\n",
    "# Evaluate performance\n",
    "train_rmse = np.sqrt(mean_squared_error(Y_train, Y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_validate_wls_with_plot(X, Y, D_diag, kappa_values, cv=5):\n",
    "    \"\"\"\n",
    "    Cross-validate to find the best kappa for regularized WLS with memory-efficient D representation\n",
    "    and plot performance.\n",
    "    \n",
    "    Parameters:\n",
    "    X : ndarray\n",
    "        Feature matrix (n x p).\n",
    "    Y : ndarray\n",
    "        Target variable (n-dimensional vector).\n",
    "    D_diag : ndarray\n",
    "        Diagonal elements of the specific risk matrix (1D array).\n",
    "    kappa_values : list\n",
    "        List of kappa values to test.\n",
    "    cv : int\n",
    "        Number of cross-validation folds.\n",
    "        \n",
    "    Returns:\n",
    "    best_kappa : float\n",
    "        Best regularization parameter.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=cv)\n",
    "    best_rmse = float(\"inf\")\n",
    "    best_kappa = None\n",
    "    kappa_rmse = []  # To store average RMSE for each kappa\n",
    "\n",
    "    for kappa in kappa_values:\n",
    "        fold_rmses = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "            D_diag_train = D_diag[train_index]  # Use 1D representation\n",
    "            \n",
    "            # Train WLS model\n",
    "            coef = regularized_wls_train(X_train, Y_train, D_diag_train, kappa)\n",
    "            Y_val_pred = predict_wls(X_val, coef)\n",
    "            \n",
    "            # Evaluate RMSE\n",
    "            fold_rmse = np.sqrt(mean_squared_error(Y_val, Y_val_pred))\n",
    "            fold_rmses.append(fold_rmse)\n",
    "        \n",
    "        avg_rmse = np.mean(fold_rmses)\n",
    "        kappa_rmse.append(avg_rmse)  # Track performance\n",
    "\n",
    "        if avg_rmse < best_rmse:\n",
    "            best_rmse = avg_rmse\n",
    "            best_kappa = kappa\n",
    "\n",
    "    # Plot RMSE vs Kappa\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(kappa_values, kappa_rmse, marker='o', linestyle='-', label=\"Cross-Validation RMSE\")\n",
    "    plt.xscale('log')  # Logarithmic scale for kappa\n",
    "    plt.xlabel(\"Regularization Parameter (Kappa)\")\n",
    "    plt.ylabel(\"Average RMSE\")\n",
    "    plt.title(\"Cross-Validation Performance for WLS\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    return best_kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kappa values to test\n",
    "kappa_values = np.logspace(-6, -2, 10)\n",
    "\n",
    "# Perform cross-validation with performance tracking\n",
    "best_kappa = cross_validate_wls_with_plot(X_train, Y_train, D_train, kappa_values)\n",
    "print(f\"Best Kappa: {best_kappa}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameter grid for the neural network\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (100, 50)],  # Different architectures\n",
    "    \"alpha\": [0.0001, 0.001],  # Regularization strength\n",
    "    \"learning_rate_init\": [0.001],  # Learning rate\n",
    "}\n",
    "\n",
    "# Initialize MLP Regressor with reduced max_iter\n",
    "mlp = MLPRegressor(max_iter=200, random_state=42)  # Set random state for reproducibility\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Detailed output during the grid search\n",
    ")\n",
    "\n",
    "# Fit the model using training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get best parameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "nn_model = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters and validation RMSE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "val_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_test_pred = nn_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "# Extract grid search results for visualization\n",
    "results = grid_search.cv_results_\n",
    "hidden_layers = results[\"param_hidden_layer_sizes\"].data\n",
    "alphas = results[\"param_alpha\"].data\n",
    "mean_rmse = np.sqrt(-results[\"mean_test_score\"])  # Convert negative MSE to RMSE\n",
    "\n",
    "# Create DataFrame for heatmap visualization\n",
    "df_results = pd.DataFrame({\n",
    "    \"Hidden Layers\": hidden_layers,\n",
    "    \"Alpha\": alphas,\n",
    "    \"Validation RMSE\": mean_rmse,\n",
    "})\n",
    "\n",
    "# Pivot the DataFrame to plot a heatmap\n",
    "pivot_table = df_results.pivot_table(index=\"Alpha\", columns=\"Hidden Layers\", values=\"Validation RMSE\")\n",
    "\n",
    "# Plot the heatmap for performance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "plt.title(\"Validation RMSE Heatmap by Alpha and Hidden Layers\")\n",
    "plt.xlabel(\"Hidden Layer Sizes\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Problem 3.\n",
    "\n",
    "**Efficient portfolio optimization** Code up the efficient formula for portfolio optimization discussed in lecture, based on the Woodbury matrix inversion lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We aim to compute optimal portfolio weights $w$ using the formula:\n",
    "\n",
    "$$\n",
    "w = \\Sigma^{-1} \\mu\n",
    "$$\n",
    "\n",
    "where $\\Sigma$ is defined as:\n",
    "\n",
    "$$\n",
    "\\Sigma = D + X F X^\\top\n",
    "$$\n",
    "\n",
    "To avoid division by zero, replace zero or near-zero values in $D$ and $F$ with a small positive constant ($1 \\times 10^{-10}$):\n",
    "\n",
    "$$\n",
    "D_\\text{diag} = \\max(D_\\text{diag}, 1 \\times 10^{-10}), \\quad F_\\text{diag} = \\max(F_\\text{diag}, 1 \\times 10^{-10})\n",
    "$$\n",
    "\n",
    "The inverses of the diagonal matrices $D$ and $F$ are calculated as:\n",
    "\n",
    "$$\n",
    "D^{-1}_\\text{diag} = \\frac{1}{D_\\text{diag}}, \\quad F^{-1}_\\text{diag} = \\frac{1}{F_\\text{diag}}\n",
    "$$\n",
    "\n",
    "Using the diagonal inverse of $D$, compute:\n",
    "\n",
    "$$\n",
    "D^{-1} X = D^{-1}_\\text{diag} \\cdot X\n",
    "$$\n",
    "\n",
    "This is achieved using element-wise multiplication, avoiding the construction of full matrices.\n",
    "\n",
    "The middle term in the Woodbury formula is:\n",
    "\n",
    "$$\n",
    "\\text{Middle Term} = \\left( F^{-1} + X^\\top D^{-1} X \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Here, the $F^{-1}$ matrix is diagonal, so we use:\n",
    "\n",
    "$$\n",
    "F^{-1} = \\text{diag}(F^{-1}_\\text{diag})\n",
    "$$\n",
    "\n",
    "Using the Woodbury matrix inversion lemma:\n",
    "\n",
    "$$\n",
    "\\Sigma^{-1} \\mu = D^{-1}_\\text{diag} \\mu - D^{-1}_\\text{diag} X \\, \\text{Middle Term} \\, X^\\top D^{-1}_\\text{diag} \\mu\n",
    "$$\n",
    "\n",
    "The resulting portfolio weights $w$ are given by:\n",
    "\n",
    "$$\n",
    "w = \\Sigma^{-1} \\mu\n",
    "$$\n",
    "\n",
    "This provides the optimal weights for the portfolio based on the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_portfolio_weights(X, F_diag, D_diag, mu):\n",
    "    \"\"\"\n",
    "    Compute optimal portfolio weights using the Woodbury matrix inversion lemma with memory-efficient diagonal representations.\n",
    "    \n",
    "    Parameters:\n",
    "    X : ndarray\n",
    "        Factor exposures matrix (n x p).\n",
    "    F_diag : ndarray\n",
    "        Diagonal elements of the factor covariance matrix (1D array, size p).\n",
    "    D_diag : ndarray\n",
    "        Diagonal elements of the specific risk covariance matrix (1D array, size n).\n",
    "    mu : ndarray\n",
    "        Expected returns (n-dimensional vector).\n",
    "    \n",
    "    Returns:\n",
    "    w : ndarray\n",
    "        Optimal portfolio weights (n-dimensional vector).\n",
    "    \"\"\"\n",
    "    # Replace zero or near-zero values with small positive values for stability\n",
    "    D_diag = np.where(D_diag > 0, D_diag, 1e-10)\n",
    "    F_diag = np.where(F_diag > 0, F_diag, 1e-10)\n",
    "    \n",
    "    # Compute D^{-1} and F^{-1}\n",
    "    D_inv_diag = 1 / D_diag\n",
    "    F_inv_diag = 1 / F_diag\n",
    "    \n",
    "    # Compute D^{-1} X efficiently\n",
    "    D_inv_X = D_inv_diag[:, None] * X  # Element-wise multiplication\n",
    "    \n",
    "    # Compute the middle term using pseudo-inverse for stability\n",
    "    middle_term = np.linalg.pinv(np.diag(F_inv_diag) + X.T @ D_inv_X)\n",
    "    \n",
    "    # Compute Sigma^{-1} using the Woodbury formula\n",
    "    Sigma_inv_mu = D_inv_diag * mu - (D_inv_diag * (X @ middle_term @ (X.T @ (D_inv_diag * mu))))\n",
    "    \n",
    "    return Sigma_inv_mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Problem 4.\n",
    "\n",
    "**Putting it all together** Using the helpful code example above, and using the output of the function $ f $ as your final alpha factor, construct a backtest of a portfolio optimization strategy. In other words, compute the optimal portfolio each day, and dot product it with Ret to get the pre-tcost 1-day profit for each day. Use the previous problem to speed things up. Create time-series plots of the long market value, short market value, and cumulative profit of this portfolio sequence. Also plot the daily risk, in dollars, of your portfolios and the percent of the risk that is idiosyncratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_with_train_test(\n",
    "    train_panel, test_panel, Y_train, X_train, D_train, candidate_alphas, compute_portfolio_weights, covariance\n",
    "):\n",
    "    \"\"\"\n",
    "    Backtest portfolio optimization strategy using WLS with train-test data.\n",
    "\n",
    "    Parameters:\n",
    "    train_panel : DataFrame\n",
    "        Training panel data.\n",
    "    test_panel : DataFrame\n",
    "        Test panel data.\n",
    "    Y_train : ndarray\n",
    "        Training target variable (residual returns).\n",
    "    X_train : ndarray\n",
    "        Training alpha factors.\n",
    "    D_train : ndarray\n",
    "        Training specific risk diagonal.\n",
    "    candidate_alphas : list\n",
    "        List of alpha factors to use.\n",
    "    compute_portfolio_weights : function\n",
    "        Function to compute portfolio weights.\n",
    "    covariance : dict\n",
    "        Dictionary containing factor covariance data by date.\n",
    "\n",
    "    Returns:\n",
    "    results : dict\n",
    "        Dictionary containing cumulative profit, daily profits, risks, market values, and optimized kappa.\n",
    "    \"\"\"\n",
    "    # Step 1: Cross-validation to find the best kappa\n",
    "    kappa_values = np.logspace(-6, -2, 10)  # Range of kappa values to test\n",
    "    best_kappa = cross_validate_wls_with_plot(X_train, Y_train, D_train, kappa_values)\n",
    "    print(f\"Best Kappa from Cross-Validation: {best_kappa}\")\n",
    "\n",
    "    # Step 2: Train the WLS model on the training data\n",
    "    coefficients = regularized_wls_train(X_train, Y_train, D_train, best_kappa)\n",
    "\n",
    "    # Step 3: Backtest on the test panel\n",
    "    cumulative_profit = 0\n",
    "    daily_profits = []\n",
    "    cumulative_profits = []\n",
    "    daily_risks = []\n",
    "    idiosyncratic_risks = []\n",
    "    idiosyncratic_risk_percentages = []\n",
    "    long_market_values = []\n",
    "    short_market_values = []\n",
    "\n",
    "    grouped = test_panel.groupby(level=\"Date\")\n",
    "\n",
    "    for date, df in grouped:\n",
    "        # Compute dynamic risk exposures and factor covariance diagonal\n",
    "        rske = risk_exposures(df)  # Compute risk exposure matrix (X)\n",
    "        F_diag = diagonal_factor_cov(date, rske)  # Compute factor covariance diagonal using covariance\n",
    "\n",
    "        # Extract necessary data\n",
    "        R = df[\"Y\"].to_numpy()  # Residual returns\n",
    "        X_full = np.asarray(rske)  # Full risk exposure matrix\n",
    "        D_diag = np.asarray((df[\"SpecRisk\"] / (100 * np.sqrt(252))) ** 2)  # Specific risk diagonal\n",
    "\n",
    "        # Generate alphas (predicted returns) from the trained WLS model\n",
    "        X = df[candidate_alphas].to_numpy()  # Extract alpha features\n",
    "        mu = predict_wls(X, coefficients)  # Predicted returns\n",
    "\n",
    "        # Compute portfolio weights using the Woodbury formula\n",
    "        weights = compute_portfolio_weights(X_full, F_diag, D_diag, mu)\n",
    "\n",
    "        # Normalize weights for market value calculations\n",
    "        total_exposure = np.sum(np.abs(weights))\n",
    "        normalized_weights = weights / total_exposure\n",
    "\n",
    "        # Compute daily profit (pre-tcost)\n",
    "        daily_profit = np.dot(weights, R)\n",
    "        daily_profits.append(daily_profit)\n",
    "\n",
    "        # Update cumulative profit\n",
    "        cumulative_profit += daily_profit\n",
    "        cumulative_profits.append(cumulative_profit)\n",
    "\n",
    "        # Compute risk metrics\n",
    "        factor_covariance = X_full @ F_diag @ X_full.T  # Corrected covariance computation\n",
    "        total_covariance = np.diag(D_diag) + factor_covariance\n",
    "        portfolio_risk = np.sqrt(weights.T @ total_covariance @ weights)\n",
    "        idiosyncratic_risk = np.sqrt(weights.T @ np.diag(D_diag) @ weights)\n",
    "        idiosyncratic_risk_percentage = (idiosyncratic_risk / portfolio_risk) * 100\n",
    "\n",
    "        # Store risks\n",
    "        daily_risks.append(portfolio_risk)\n",
    "        idiosyncratic_risks.append(idiosyncratic_risk)\n",
    "        idiosyncratic_risk_percentages.append(idiosyncratic_risk_percentage)\n",
    "\n",
    "        # Compute long and short market values\n",
    "        long_value = np.sum(normalized_weights[normalized_weights > 0])\n",
    "        short_value = np.sum(np.abs(normalized_weights[normalized_weights < 0]))\n",
    "        long_market_values.append(long_value)\n",
    "        short_market_values.append(short_value)\n",
    "\n",
    "    # Extract the dates for plotting\n",
    "    dates = [date for date, _ in grouped]\n",
    "\n",
    "    return {\n",
    "        \"daily_profits\": daily_profits,\n",
    "        \"cumulative_profits\": cumulative_profits,\n",
    "        \"daily_risks\": daily_risks,\n",
    "        \"idiosyncratic_risks\": idiosyncratic_risks,\n",
    "        \"idiosyncratic_risk_percentages\": idiosyncratic_risk_percentages,\n",
    "        \"long_market_values\": long_market_values,\n",
    "        \"short_market_values\": short_market_values,\n",
    "        \"best_kappa\": best_kappa,\n",
    "        \"dates\": dates,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the backtest using the provided inputs\n",
    "results_train_test = backtest_with_train_test(\n",
    "    train_panel=train_panel,\n",
    "    test_panel=test_panel,\n",
    "    Y_train=Y_train,\n",
    "    X_train=X_train,\n",
    "    D_train=D_train,\n",
    "    candidate_alphas=candidate_alphas,\n",
    "    compute_portfolio_weights=compute_portfolio_weights,\n",
    "    covariance=covariance,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract results for plotting\n",
    "cumulative_profits_train_test = results_train_test[\"cumulative_profits\"]\n",
    "daily_profits_train_test = results_train_test[\"daily_profits\"]\n",
    "daily_risks = results_train_test[\"daily_risks\"]\n",
    "idiosyncratic_risks = results_train_test[\"idiosyncratic_risks\"]\n",
    "idiosyncratic_risk_percentages = results_train_test[\"idiosyncratic_risk_percentages\"]\n",
    "long_market_values = results_train_test[\"long_market_values\"]\n",
    "short_market_values = results_train_test[\"short_market_values\"]\n",
    "best_kappa_train_test = results_train_test[\"best_kappa\"]\n",
    "dates_train_test = results_train_test[\"dates\"]\n",
    "\n",
    "# Plot Long and Short Market Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_train_test, long_market_values, label=\"Long Market Value\")\n",
    "plt.plot(dates_train_test, short_market_values, label=\"Short Market Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Market Value\")\n",
    "plt.title(\"Long and Short Market Values Over Time\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Cumulative Profit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_train_test, cumulative_profits_train_test, label=\"Cumulative Profit (Train-Test)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Profit\")\n",
    "plt.title(f\"Cumulative Profit Over Time (Train-Test, Kappa={best_kappa_train_test:.1e})\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Daily Profits\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_train_test, daily_profits_train_test, label=\"Daily Profit (Train-Test)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Daily Profit\")\n",
    "plt.title(f\"Daily Pre-Tcost Profit Over Time (Train-Test, Kappa={best_kappa_train_test:.1e})\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Daily Risks\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_train_test, daily_risks, label=\"Daily Total Risk\")\n",
    "plt.plot(dates_train_test, idiosyncratic_risks, label=\"Idiosyncratic Risk\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Risk\")\n",
    "plt.title(\"Daily Portfolio Risks Over Time\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Idiosyncratic Risk Percentage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_train_test, idiosyncratic_risk_percentages, label=\"Idiosyncratic Risk Percentage\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Idiosyncratic Risk (%)\")\n",
    "plt.title(\"Daily Idiosyncratic Risk Percentage Over Time\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parameter grid for the neural network\n",
    "param_grid = {\n",
    "    \"hidden_layer_sizes\": [(50,), (100,), (100, 50)],  # Different architectures\n",
    "    \"alpha\": [0.0001, 0.001],  # Regularization strength\n",
    "    \"learning_rate_init\": [0.001],  # Learning rate\n",
    "}\n",
    "\n",
    "# Initialize MLP Regressor with reduced max_iter\n",
    "mlp = MLPRegressor(max_iter=200, random_state=42)  # Set random state for reproducibility\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    verbose=1,  # Detailed output during the grid search\n",
    ")\n",
    "\n",
    "# Fit the model using training data\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get best parameters and the corresponding model\n",
    "best_params = grid_search.best_params_\n",
    "nn_model = grid_search.best_estimator_\n",
    "\n",
    "# Print best parameters and validation RMSE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "val_rmse = np.sqrt(-grid_search.best_score_)\n",
    "print(f\"Validation RMSE: {val_rmse}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_test_pred = nn_model.predict(X_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(Y_test, Y_test_pred))\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "\n",
    "# Extract grid search results for visualization\n",
    "results = grid_search.cv_results_\n",
    "hidden_layers = results[\"param_hidden_layer_sizes\"].data\n",
    "alphas = results[\"param_alpha\"].data\n",
    "mean_rmse = np.sqrt(-results[\"mean_test_score\"])  # Convert negative MSE to RMSE\n",
    "\n",
    "# Create DataFrame for heatmap visualization\n",
    "df_results = pd.DataFrame({\n",
    "    \"Hidden Layers\": hidden_layers,\n",
    "    \"Alpha\": alphas,\n",
    "    \"Validation RMSE\": mean_rmse,\n",
    "})\n",
    "\n",
    "# Pivot the DataFrame to plot a heatmap\n",
    "pivot_table = df_results.pivot_table(index=\"Alpha\", columns=\"Hidden Layers\", values=\"Validation RMSE\")\n",
    "\n",
    "# Plot the heatmap for performance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".4f\", cmap=\"viridis\")\n",
    "plt.title(\"Validation RMSE Heatmap by Alpha and Hidden Layers\")\n",
    "plt.xlabel(\"Hidden Layer Sizes\")\n",
    "plt.ylabel(\"Alpha\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backtest_with_nn_model(\n",
    "    test_panel, \n",
    "    nn_model, \n",
    "    candidate_alphas, \n",
    "    compute_portfolio_weights, \n",
    "    covariance\n",
    "):\n",
    "    \"\"\"\n",
    "    Backtest portfolio optimization strategy using a pretrained neural network model.\n",
    "\n",
    "    Parameters:\n",
    "    test_panel : DataFrame\n",
    "        Test panel data.\n",
    "    nn_model : sklearn.neural_network.MLPRegressor\n",
    "        Pretrained neural network model.\n",
    "    candidate_alphas : list\n",
    "        List of alpha factors to use.\n",
    "    compute_portfolio_weights : function\n",
    "        Function to compute portfolio weights.\n",
    "    covariance : dict\n",
    "        Dictionary containing factor covariance data by date.\n",
    "\n",
    "    Returns:\n",
    "    results : dict\n",
    "        Dictionary containing cumulative profit, daily profits, risks, and market values.\n",
    "    \"\"\"\n",
    "    cumulative_profit = 0\n",
    "    daily_profits = []\n",
    "    cumulative_profits = []\n",
    "    daily_risks = []\n",
    "    idiosyncratic_risks = []\n",
    "    idiosyncratic_risk_percentages = []\n",
    "    long_market_values = []\n",
    "    short_market_values = []\n",
    "\n",
    "    grouped = test_panel.groupby(level=\"Date\")\n",
    "\n",
    "    for date, df in grouped:\n",
    "        # Compute risk exposures and factor covariance diagonal\n",
    "        rske = risk_exposures(df)  # Risk exposure matrix\n",
    "        F_diag = diagonal_factor_cov(date, rske)  # Factor covariance diagonal\n",
    "        X_full = np.asarray(rske)  # Full risk exposure matrix\n",
    "        D_diag = np.asarray((df[\"SpecRisk\"] / (100 * np.sqrt(252))) ** 2)  # Specific risk diagonal\n",
    "\n",
    "        # Predict expected returns using the pretrained neural network\n",
    "        X = df[candidate_alphas].to_numpy()  # Extract alpha features\n",
    "        mu = np.abs(nn_model.predict(X))  # Predicted returns\n",
    "\n",
    "        # Compute portfolio weights using the Woodbury formula\n",
    "        weights = compute_portfolio_weights(X_full, F_diag, D_diag, mu)\n",
    "\n",
    "        # Normalize weights for market value calculations\n",
    "        total_exposure = np.sum(np.abs(weights))\n",
    "        normalized_weights = weights / total_exposure\n",
    "\n",
    "        # Compute daily profit (pre-tcost)\n",
    "        R = df[\"Y\"].to_numpy()  # Residual returns\n",
    "        daily_profit = np.dot(weights, R)\n",
    "        daily_profits.append(daily_profit)\n",
    "\n",
    "        # Update cumulative profit\n",
    "        cumulative_profit += daily_profit\n",
    "        cumulative_profits.append(cumulative_profit)\n",
    "\n",
    "        # Compute risk metrics\n",
    "        factor_covariance = X_full @ F_diag @ X_full.T\n",
    "        total_covariance = np.diag(D_diag) + factor_covariance\n",
    "        portfolio_risk = np.sqrt(weights.T @ total_covariance @ weights)\n",
    "        idiosyncratic_risk = np.sqrt(weights.T @ np.diag(D_diag) @ weights)\n",
    "        idiosyncratic_risk_percentage = (idiosyncratic_risk / portfolio_risk) * 100\n",
    "\n",
    "        # Compute long and short market values\n",
    "        long_value = np.sum(normalized_weights[normalized_weights > 0])\n",
    "        short_value = np.sum(np.abs(normalized_weights[normalized_weights < 0]))\n",
    "\n",
    "        # Store metrics\n",
    "        daily_risks.append(portfolio_risk)\n",
    "        idiosyncratic_risks.append(idiosyncratic_risk)\n",
    "        idiosyncratic_risk_percentages.append(idiosyncratic_risk_percentage)\n",
    "        long_market_values.append(long_value)\n",
    "        short_market_values.append(short_value)\n",
    "\n",
    "    # Extract the dates for plotting\n",
    "    dates = [date for date, _ in grouped]\n",
    "\n",
    "    return {\n",
    "        \"daily_profits\": daily_profits,\n",
    "        \"cumulative_profits\": cumulative_profits,\n",
    "        \"daily_risks\": daily_risks,\n",
    "        \"idiosyncratic_risks\": idiosyncratic_risks,\n",
    "        \"idiosyncratic_risk_percentages\": idiosyncratic_risk_percentages,\n",
    "        \"long_market_values\": long_market_values,\n",
    "        \"short_market_values\": short_market_values,\n",
    "        \"dates\": dates,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the backtest using the pretrained neural network\n",
    "results_nn_backtest = backtest_with_nn_model(\n",
    "    test_panel=test_panel,\n",
    "    nn_model=nn_model,  # Pretrained neural network model\n",
    "    candidate_alphas=candidate_alphas,\n",
    "    compute_portfolio_weights=compute_portfolio_weights,\n",
    "    covariance=covariance,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract results for plotting\n",
    "cumulative_profits_nn = results_nn_backtest[\"cumulative_profits\"]\n",
    "daily_profits_nn = results_nn_backtest[\"daily_profits\"]\n",
    "daily_risks_nn = results_nn_backtest[\"daily_risks\"]\n",
    "idiosyncratic_risks_nn = results_nn_backtest[\"idiosyncratic_risks\"]\n",
    "idiosyncratic_risk_percentages_nn = results_nn_backtest[\"idiosyncratic_risk_percentages\"]\n",
    "long_market_values_nn = results_nn_backtest[\"long_market_values\"]\n",
    "short_market_values_nn = results_nn_backtest[\"short_market_values\"]\n",
    "dates_nn = results_nn_backtest[\"dates\"]\n",
    "\n",
    "# Visualization\n",
    "# Plot Long and Short Market Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_nn, long_market_values_nn, label=\"Long Market Value\")\n",
    "plt.plot(dates_nn, short_market_values_nn, label=\"Short Market Value\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Market Value\")\n",
    "plt.title(\"Long and Short Market Values Over Time (NN)\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Cumulative Profit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_nn, cumulative_profits_nn, label=\"Cumulative Profit (NN)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cumulative Profit\")\n",
    "plt.title(\"Cumulative Profit Over Time (Neural Network)\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Daily Risks\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_nn, daily_risks_nn, label=\"Daily Total Risk\")\n",
    "plt.plot(dates_nn, idiosyncratic_risks_nn, label=\"Idiosyncratic Risk\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Risk\")\n",
    "plt.title(\"Daily Portfolio Risks Over Time (NN)\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Plot Idiosyncratic Risk Percentage\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dates_nn, idiosyncratic_risk_percentages_nn, label=\"Idiosyncratic Risk Percentage\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Idiosyncratic Risk (%)\")\n",
    "plt.title(\"Daily Idiosyncratic Risk Percentage Over Time (NN)\")\n",
    "plt.legend()\n",
    "for ind, label in enumerate(plt.gca().get_xticklabels()):\n",
    "    if ind % 100 == 0:\n",
    "        label.set_visible(True)\n",
    "    else:\n",
    "        label.set_visible(False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
